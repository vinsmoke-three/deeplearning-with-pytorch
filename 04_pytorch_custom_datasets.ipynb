{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d2d55c",
   "metadata": {},
   "source": [
    "# 04. Pytorch Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fb0ae",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "The data we're going to be using is a subset of the Food101 dataset.\n",
    "\n",
    "Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\n",
    "\n",
    "Can you think of 101 different foods?\n",
    "\n",
    "Can you think of a computer program to classify 101 foods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# setup path to a data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, create one ...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download pizza, steak and sushi data\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    f.write(request.content)\n",
    "\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c5d5f",
   "metadata": {},
   "source": [
    "## 2. Become one with the data (data preparation)\n",
    "\n",
    "The goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.\n",
    "\n",
    "We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\n",
    "\n",
    "To do so, we'll use Python's in-built os.walk()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75919906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "    \"\"\"Walks through dir_path returning its contents.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")\n",
    "\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5112c9",
   "metadata": {},
   "source": [
    "### 2.2 Visualize an image\n",
    "\n",
    "Let's write some code to:\n",
    "\n",
    "1. Get all of the image paths using `pathlib.Path.glob()` to find all of the files ending in .jpg.\n",
    "2. Pick a random image path using Python's `random.choice()`.\n",
    "3. Get the image class name using `pathlib.Path.parent.stem`.\n",
    "4. And since we're working with images, we'll open the random image path using `PIL.Image.open()` (PIL stands for Python Image Library).\n",
    "5. We'll then show the image and print some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# set seed\n",
    "# random.seed(42)\n",
    "\n",
    "# 1. get all image paths \n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# 2. get random image path\n",
    "random_image_path =random.choice(image_path_list)\n",
    "\n",
    "# 3. get image class from path name\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# 5. print metadata\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image heigth: {img.height}\")\n",
    "print(f\"Image width: {img.width}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b02969",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_as_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbd67a",
   "metadata": {},
   "source": [
    "## 3. Transforming data\n",
    "\n",
    "Before we can use our image data with PyTorch we need to:\n",
    "\n",
    "1. Turn it into tensors (numerical representations of our images).\n",
    "2. Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader`, we'll call these Dataset and DataLoader for short.\n",
    "\n",
    "There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on.\n",
    "\n",
    "|Problem space\t|Pre-built Datasets and Functions|\n",
    "|-|-|\n",
    "|Vision|\ttorchvision.datasets|\n",
    "|Audio\t|torchaudio.datasets|\n",
    "|Text\t|torchtext.datasets|\n",
    "|Recommendation system\t|torchrec.datasets|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e065830",
   "metadata": {},
   "source": [
    "### 3.1 Transforming data with torchvision.transforms\n",
    "We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.\n",
    "\n",
    "One of the ways we can do this is by using the torchvision.transforms module.\n",
    "\n",
    "torchvision.transforms contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes .\n",
    "\n",
    "To get experience with torchvision.transforms, let's write a series of transform steps that:\n",
    "\n",
    "1. Resize the images using transforms.Resize() (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website).\n",
    "2. Flip our images randomly on the horizontal using transforms.RandomHorizontalFlip() (this could be considered a form of data augmentation because it will artificially change our image data).\n",
    "3. Turn our images from a PIL image to a PyTorch tensor using transforms.ToTensor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893db7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write transforms for image\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "type(data_transform(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths: list, transform, n: int=3, seed: int=42):\n",
    "    \"\"\"Plots a series of random images from image_path.\n",
    "    \n",
    "    Will open n image path from image_path, transfrom them \n",
    "    with transform and plot the side by side\n",
    "\n",
    "    args:\n",
    "        images_paths(list): list of target image paths.\n",
    "        transform(PyTorch Transforms): Transforms to apply to images.\n",
    "        n(int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed(int, optional): Random seed for the random generator.default to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths,k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "            ax[0].imshow(f)\n",
    "            ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "            ax[0].axis(False)\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0)\n",
    "            ax[1].imshow(transformed_image)\n",
    "            ax[1].set_title(f\"Transformed\\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(False)\n",
    "\n",
    "            fig.suptitle(f\"class: {image_path.parent.stem}\")\n",
    "\n",
    "plot_transformed_images(image_path_list, data_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdf607",
   "metadata": {},
   "source": [
    "## 4. Option 1: Loading Image Data Using ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8539775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imageFolder to create datasets\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=data_transform,\n",
    "                                  target_transform=None\n",
    "                                  )\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                    transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\")\n",
    "print(f\"Test data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb809d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc50eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Image label: {class_names[label]}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2262835",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_permute= img.permute(1, 2, 0)\n",
    "plt.imshow(img_permute)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abaa821",
   "metadata": {},
   "source": [
    "### 4.1 turn loaded image into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=1)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=1)\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "print(img.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d64d43",
   "metadata": {},
   "source": [
    "## 5. Optional 2: Loading image data with a custom dataset\n",
    "\n",
    "|Pros of creating a custom Dataset\t|Cons of creating a custom Dataset|\n",
    "|-|-|\n",
    "|Can create a Dataset out of almost anything.\t|Even though you could create a Dataset out of almost anything, it doesn't mean it will work.|\n",
    "|Not limited to PyTorch pre-built Dataset functions.\t|Using a custom Dataset often results in writing more code, which could be prone to errors or performance issues.|\n",
    "\n",
    "To see this in action, let's work towards replicating torchvision.datasets.ImageFolder() by subclassing torch.utils.data.Dataset (the base class for all Dataset's in PyTorch).\n",
    "\n",
    "We'll start by importing the modules we need:\n",
    "\n",
    "* Python's `os` for dealing with directories (our data is stored in directories).\n",
    "* Python's `pathlib` for dealing with filepaths (each of our images has a unique filepath).\n",
    "* `torch` for all things PyTorch.\n",
    "* PIL's `Image` class for loading images.\n",
    "* `torch.utils.data.Dataset` to subclass and create our own custom Dataset.\n",
    "* `torchvision.transforms` to turn our images into tensors.\n",
    "* Various types from Python's `typing` module to add type hints to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10719f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of torchvision.datasets.ImageFolder()\n",
    "train_data.classes, train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d95eb1",
   "metadata": {},
   "source": [
    "### 5.1 Createing a helper function to get class names\n",
    "\n",
    "Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.\n",
    "\n",
    "To do so, we'll:\n",
    "\n",
    "* Get the class names using os.scandir() to traverse a target directory (ideally the directory is in standard image classification format).\n",
    "* Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).\n",
    "* Turn the class names into a dictionary of numerical labels, one for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup path for target directory\n",
    "target_directory = train_dir\n",
    "print(f\"Target directory:{target_directory}\")\n",
    "\n",
    "# get the class names from the target directory\n",
    "class_names_found = sorted([entry.name for entry in list(os.scandir(target_directory))])\n",
    "print(class_names_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50532b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(os.scandir(target_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29679628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted([entry.name for entry in list(os.scandir(target_directory))])\n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Counldn't find any classes in {directory}.\")\n",
    "    # 3. Create a dictionary of index labels\n",
    "    class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "find_classes(target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8502e3",
   "metadata": {},
   "source": [
    "### 5.2 Create a custom Dataset to replicate ImageFolder\n",
    "\n",
    "1. Subclass `torch.utils.data.Dataset`.\n",
    "2. Initialize our subclass with a `targ_dir` parameter (the target data directory) and transform parameter (so we have the option to transform our data if needed).\n",
    "3. Create several attributes for `paths` (the paths of our target images), `transform` (the transforms we might like to use, this can be `None`), `classes` and `class_to_idx` (from our `find_classes()` function).\n",
    "4. Create a function to load images from file and return them, this could be using `PIL` or `torchvision.io` (for input/output of vision data).\n",
    "5. Overwrite the `__len__` method of `torch.utils.data.Dataset` to return the number of samples in the `Dataset`, this is recommended but not required. This is so you can call len(`Dataset`).\n",
    "6. Overwrite the `__getitem__` method of `torch.utils.data.Dataset` to return a single sample from the `Dataset`, this is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    # 2. Initialize our custom dataset\n",
    "    def __init__(self, target_directory: str, transform=None) -> None:\n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(target_directory).glob(\"*/*.jpg\"))\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(target_directory)\n",
    "\n",
    "    # 4. Create a function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "    \n",
    "    # 5. Overwrite the __len__() method\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label(X, y)\"\n",
    "        img = self.load_image(index)\n",
    "        # expects path in data_folder/class_name/image.jpeg\n",
    "        class_name = self.paths[index].parent.name\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform of if necessary\n",
    "        if self.transform:\n",
    "            # return data, label(X, y)\n",
    "            return self.transform(img), class_idx\n",
    "        else:\n",
    "            return img, class_idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment train data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom = ImageFolderCustom(target_directory=train_dir,\n",
    "                                      transform=train_transform)\n",
    "test_data_custom = ImageFolderCustom(target_directory=test_dir,\n",
    "                                     transform=test_transform)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(train_data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b96f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data), len(test_data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_custom.classes == train_data.classes)\n",
    "print(train_data_custom.class_to_idx)\n",
    "print(test_data_custom.classes)\n",
    "print(test_data_custom.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8ed27",
   "metadata": {},
   "source": [
    "### 5.3 Create a function to display random images\n",
    "\n",
    "Let's create a helper function called display_random_images() that helps us visualize images in our Dataset's.\n",
    "\n",
    "Specifically, it'll:\n",
    "\n",
    "1. Take in a Dataset and a number of other parameters such as classes (the names of our target classes), the number of images to display (n) and a random seed.\n",
    "2. To prevent the display getting out of hand, we'll cap n at 10 images.\n",
    "3. Set the random seed for reproducible plots (if seed is set).\n",
    "4. Get a list of random sample indexes (we can use Python's random.sample() for this) to plot.\n",
    "5. Setup a matplotlib plot.\n",
    "6. Loop through the random sample indexes found in step 4 and plot them with matplotlib.\n",
    "7. Make sure the sample images are of shape HWC (height, width, color channels) so we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cff0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a function to take in a dataset\n",
    "def display_random_images(dataset: torch.utils.data.Dataset,\n",
    "                          classes: List[str] = None,\n",
    "                          n: int = 10,\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "    \n",
    "    # 2. Adjust display if n is too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "\n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k = n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples\n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "        # 7. Adjust image tensor shape for plotting\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(targ_image_adjust)\n",
    "        plt.axis(False)\n",
    "        if classes:\n",
    "            title = f\"class: {classes[targ_label]}\"\n",
    "            if display_shape:\n",
    "                title = title + f\"\\nshape:{targ_image_adjust.shape}\"\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760449b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display random images from ImageFolder created dataset\n",
    "display_random_images(train_data,\n",
    "                      class_names,\n",
    "                      5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display random images from ImageFolder created dataset\n",
    "display_random_images(train_data_custom,\n",
    "                      class_names,\n",
    "                      5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcde9a2",
   "metadata": {},
   "source": [
    "### 5.4 Turn custom loaded images into dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaddae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKS = 0\n",
    "print(NUM_WORKS)\n",
    "train_dataloader_custom = DataLoader(\n",
    "    dataset=train_data_custom,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKS\n",
    ")\n",
    "test_dataloader_custom = DataLoader(\n",
    "    dataset=test_data_custom,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKS\n",
    ")\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_custom, img_label = next(iter(train_dataloader_custom))\n",
    "img_custom.shape, img_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521aa34",
   "metadata": {},
   "source": [
    "## 6. other forms of transforms(data augmentation)\n",
    "\n",
    "Data augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfeb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image paths\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "plot_transformed_images(image_paths=image_path_list,\n",
    "                        transform=train_transform,\n",
    "                        n=3,\n",
    "                        seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbd775",
   "metadata": {},
   "source": [
    "## 7. Model 0: TinyVGG withput data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87288b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample transform\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12dcb8",
   "metadata": {},
   "source": [
    "### 7.1 Creating transforms and loading data for Model 0\n",
    "\n",
    "1. Load the data, turning each of our training and test folders first into a Dataset with torchvision.datasets.ImageFolder()\n",
    "2. Then into a DataLoader using torch.utils.data.DataLoader().\n",
    "We'll set the batch_size=32 and num_workers to as many CPUs on our machine (this will depend on what machine you're using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform data\n",
    "from torchvision import datasets\n",
    "train_data_sample = datasets.ImageFolder(root=train_dir,\n",
    "                                         transform=simple_transform)\n",
    "test_data_sample = datasets.ImageFolder(root=test_dir,\n",
    "                                        transform=simple_transform)\n",
    "\n",
    "# turn data into dataloader\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# setup batch size and number of workers\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKS = 0\n",
    "\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKS} workers\")\n",
    "\n",
    "# Create DataLoader's\n",
    "train_dataloader_simple = DataLoader(dataset=train_data_sample,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=NUM_WORKS)\n",
    "test_dataloader_simple = DataLoader(dataset=test_data_sample,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=NUM_WORKS)\n",
    "\n",
    "train_dataloader_simple, test_dataloader_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7bf40",
   "metadata": {},
   "source": [
    "### 7.2 Create TinyVGG model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=2,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=2,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=1,\n",
    "                         padding=0)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=2,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=2,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=10*33*33,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed67187",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, hidden_units=10, output_shape=10).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724bae2",
   "metadata": {},
   "source": [
    "### 7.3 Try a forward pass on a single image (to test the model)\n",
    "To do a forward pass on a single image, let's:\n",
    "\n",
    "1. Get a batch of images and labels from the DataLoader.\n",
    "2. Get a single image from the batch and unsqueeze() the image so it has a batch size of 1 (so its shape fits the model).\n",
    "3. Perform inference on a single image (making sure to send the image to the target device).\n",
    "4. Print out what's happening and convert the model's raw output logits to prediction probabilities with torch.softmax() (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with torch.argmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef22396",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, label_batch = next(iter(train_dataloader_simple))\n",
    "print(img_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "img_single.shape, label_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37776307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    preds = model_0(img_single.to(device))\n",
    "\n",
    "print(preds.shape)\n",
    "print(torch.softmax(preds, dim=1).argmax(dim=1))\n",
    "print(label_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0456912",
   "metadata": {},
   "source": [
    "### 7.4 Use torchinfo to get an idea of the shapes going through our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44304a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model=model_0, input_size=[1, 3, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366a498",
   "metadata": {},
   "source": [
    "### 7.5 Create train & test loop function\n",
    "Specifically, we're going to make three functions:\n",
    "\n",
    "* train_step() - takes in a model, a DataLoader, a loss function and an optimizer and trains the model on the DataLoader.\n",
    "* test_step() - takes in a model, a DataLoader and a loss function and evaluates the model on the DataLoader.\n",
    "* train() - performs 1. and 2. together for a given number of epochs and returns a results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from common_function import train_step, test_step\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: nn.Module,\n",
    "               train_dataloader: torch.utils.data.DataLoader,\n",
    "               test_dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               loss_fn: torch.nn.Module=nn.CrossEntropyLoss(),\n",
    "               epochs: int=5):\n",
    "    # 2. create empty results dictionary\n",
    "    results = {\n",
    "        \"train_loss\":[],\n",
    "        \"train_acc\":[],\n",
    "        \"test_loss\":[],\n",
    "        \"test_acc\":[]\n",
    "    }\n",
    "\n",
    "    # 3. loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device\n",
    "                                           )\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "        # 5. Update results dictionary\n",
    "        # Ensure all data is moved to CPU and converted to float for storage\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6138d",
   "metadata": {},
   "source": [
    "### 7.7 train and evaluate Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "\n",
    "# set epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# recreate an instance of TinyVGG\n",
    "model_0 = TinyVGG(input_shape=3,\n",
    "                  hidden_units=10,\n",
    "                  output_shape=len(train_data.classes)).to(device=device)\n",
    "\n",
    "# setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
    "                             lr=0.001)\n",
    "\n",
    "# start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# train model_0\n",
    "model_0_result = train(model=model_0,\n",
    "                       train_dataloader=train_dataloader_simple,\n",
    "                       test_dataloader=test_dataloader_simple,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=NUM_EPOCHS)\n",
    "# end the timer and print out how long it took\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Total training time: {end_time-start_time:.3f} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661899a",
   "metadata": {},
   "source": [
    "### 7.8 plot the loss curves of Model 0\n",
    "From the print outs of our model_0 training, it didn't look like it did too well.\n",
    "\n",
    "But we can further evaluate it by plotting the model's loss curves.\n",
    "\n",
    "Loss curves show the model's results over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model_0_results keys\n",
    "model_0_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d387c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    # Get the loss values of the results dictionary\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "    \n",
    "    # get the accuracy values of the results dictionary\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_acc')\n",
    "    plt.plot(epochs, test_accuracy, label='test_acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results=model_0_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b19d3",
   "metadata": {},
   "source": [
    "## 8. What should an ideal loss curve look like\n",
    "https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves?hl=ja\n",
    "\n",
    "### 8.1 How to deal with overfitting¶\n",
    "\n",
    "|Method to prevent overfitting\t|What is it?|\n",
    "|-|-|\n",
    "|Get more data\t|Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.|\n",
    "|Simplify your model\t|If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.|\n",
    "|Use data augmentation\t|Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.|\n",
    "|Use transfer learning\t|Transfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.|\n",
    "|Use dropout layers\t|Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See torch.nn.Dropout() for more.|\n",
    "|Use learning rate decay\t|The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller you'll want your weight updates to be.|\n",
    "|Use early stopping\t|Early stopping stops model training before it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior).|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cd9d3",
   "metadata": {},
   "source": [
    "### 8.2 How to deal with underfitting\n",
    "\n",
    "|Method to prevent underfitting|\tWhat is it?|\n",
    "|-|-|\n",
    "|Add more layers/units to your model\t|If your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers.|\n",
    "|Tweak the learning rate\t|Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens.|\n",
    "|Use transfer learning\t|Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem.|\n",
    "T|rain for longer\t|Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance.|\n",
    "|Use less regularization\t|Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ab891",
   "metadata": {},
   "source": [
    "## 9. Model 1: TinyVGG with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa9d6d",
   "metadata": {},
   "source": [
    "### 9.1 Create transform with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f9367",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create training transform with TriviailAugment\n",
    "from torchvision import transforms\n",
    "train_trainsform_trivial = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_trainsform_simple = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d8334",
   "metadata": {},
   "source": [
    "### 9.2 Create train and test Dataset's and DataLoader's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7054c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn image folders into Datasets\n",
    "from torchvision import datasets\n",
    "train_data_augmented = datasets.ImageFolder(root=train_dir,\n",
    "                                          transform=train_trainsform_trivial)\n",
    "test_data_simple = datasets.ImageFolder(root=test_dir,\n",
    "                                          transform=test_trainsform_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our Dataset into Data+pader\n",
    "import os\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKS=1\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader_argumented = DataLoader(dataset=train_data_augmented,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=NUM_WORKS)\n",
    "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=NUM_WORKS)\n",
    "train_dataloader_argumented, test_dataloader_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ee384",
   "metadata": {},
   "source": [
    "### 9.3 Construct and train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_1 and send it to the target device\n",
    "torch.manual_seed(42)\n",
    "model_1 = TinyVGG(input_shape=3,\n",
    "                  hidden_units=10,\n",
    "                  output_shape=len(train_data.classes)).to(device=device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a223ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "\n",
    "# set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n",
    "\n",
    "# start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# train model_1\n",
    "\n",
    "model_1_results = train(model=model_1,\n",
    "                        train_dataloader=train_dataloader_argumented,\n",
    "                        test_dataloader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "end_time = timer()\n",
    "print(f\"total training time: {end_time-start_time:.3f} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0efca",
   "metadata": {},
   "source": [
    "### 9.4 Plot the loss curves of model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd04d",
   "metadata": {},
   "source": [
    "## 10. Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_0_df = pd.DataFrame(model_0_result)\n",
    "model_1_df = pd.DataFrame(model_1_results)\n",
    "model_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a plot \n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(model_0_df))\n",
    "\n",
    "# Plot train loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot train accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
    "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
    "plt.title(\"Train Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55813176",
   "metadata": {},
   "source": [
    "## 11. Make a prediction on a custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dacfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# setup custom image path\n",
    "custom_image_path = data_path / \"custom-image-sushi.jpeg\"\n",
    "type(custom_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
    "custom_image = custom_image[:3]  # RGB\n",
    "custom_image /= 255\n",
    "# print out image data\n",
    "#print(f\"Custom image tensor:\\n{custom_image_uint8}\")\n",
    "print(custom_image.shape)\n",
    "print(custom_image.dtype)\n",
    "print(custom_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce6d71",
   "metadata": {},
   "source": [
    "### 11.2 Predicting on custom images with a trained pytorch model\n",
    "Our model was trained on images with shape [3, 64, 64], whereas our custom image is currently [4, 998, 944].\n",
    "\n",
    "How could we make sure our custom image is the same shape as the images our model was trained on?\n",
    "\n",
    "Are there any torchvision.transforms that could help?\n",
    "\n",
    "Before we answer that question, let's plot the image with matplotlib to make sure it looks okay, remember we'll have to permute the dimensions from CHW to HWC to suit matplotlib's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943016b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom image\n",
    "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
    "plt.title(f\"Image shape: {custom_image.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de768717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transform pipeline to resize image\n",
    "custom_image_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64))\n",
    "])\n",
    "\n",
    "custom_image_transformed = custom_image_transform(custom_image)\n",
    "\n",
    "print(custom_image_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom image\n",
    "plt.imshow(custom_image_transformed.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
    "plt.title(f\"Image shape: {custom_image_transformed.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d176bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0).to(device)\n",
    "    print(custom_image_transformed_with_batch_size.shape)\n",
    "    \n",
    "    custom_image_pred = model_0(custom_image_transformed_with_batch_size).to(device)\n",
    "\n",
    "custom_image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_label = torch.softmax(custom_image_pred, dim=1).argmax(dim=1)\n",
    "print(class_names[custom_image_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7877c76",
   "metadata": {},
   "source": [
    "### 11.3 Putting custom image prediction together: building a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_plot_image(model: torch.nn.Module,\n",
    "                        image_path: str,\n",
    "                        class_names: List[str]=None,\n",
    "                        transform=None,\n",
    "                        device=\"cpu\"):\n",
    "    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n",
    "    # 1. Load in image and convert the tensor values to float32\n",
    "    target_image = torchvision.io.read_image(image_path).type(torch.float32)\n",
    "\n",
    "    # 2. Divide the image pixel values by 255 to get them between [0,1]\n",
    "    target_image /= 255\n",
    "\n",
    "    # 3. Transform if necessary\n",
    "    if target_image.shape[0] == 4:\n",
    "        target_image = target_image[:3]\n",
    "    if transform:\n",
    "        target_image = transform(target_image)\n",
    "    \n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # add an extra dimension to the image\n",
    "        target_image = target_image.unsqueeze(dim=0)\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(target_image.to(device))\n",
    "    \n",
    "    # 6. Convert logits to prediction probabilities\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 7. Convert prediction probabilities to prediction probability\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # 8. Plot the image alongside the prediction and predcition probability\n",
    "    plt.imshow(target_image.squeeze().permute(1, 2, 0))\n",
    "    if class_names:\n",
    "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    else: \n",
    "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred on our custom image\n",
    "pred_and_plot_image(model=model_0,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names,\n",
    "                    transform=custom_image_transform,\n",
    "                    device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
